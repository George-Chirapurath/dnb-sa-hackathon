{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a70c0ec-d3fd-4148-bfe6-4352c74a6505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Generating Synthetic Data for Data Quality Testing\n",
    "This section demonstrates how to set up and generate sample data using the `dbldatagen` library. The data includes representative PII-like fields and will later be used for data quality checks.\n",
    "\n",
    "We configure the synthetic data generator to produce rows with a fields such as email, IP address, location, phone number, SSN, and credit card information. Templates define the format of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9746e65b-c1a9-4851-8ea2-b876af69b255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install dbldatagen, a library for generating synthetic test data\n",
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b75663-3765-4d17-a7be-a76aa52365f9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756227570047}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_spec = (\n",
    "    dg.DataGenerator(\n",
    "        sparkSession=spark,\n",
    "        name=\"test_data_set1\",\n",
    "        rows=100000,\n",
    "        partitions=4,\n",
    "        randomSeedMethod=\"hash_fieldname\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"email\", \"string\",\n",
    "        template=r'\\w.\\w@\\w.com|\\w@\\w.co.uk'\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ip_addr\", \"string\",\n",
    "        template=r'\\n.\\n.\\n.\\n'\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"location\", \"string\",\n",
    "        values=['Seattle', 'New York', 'Los Angeles', 'Chicago', 'San Francisco'],\n",
    "        random=True\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"phone\", \"string\",\n",
    "        template=r'(ddd)-ddd-dddd'\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ssn\", \"string\",\n",
    "        template=r'ddd-dd-dddd'\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"CC_number\", \"string\",\n",
    "        template=r'dddddddddddddddd'\n",
    "    )\n",
    "    .withColumn (\n",
    "        \"product_purchased\", \"string\",\n",
    "        values=['product1', 'product2', 'product3'],\n",
    "        random=True\n",
    "    )\n",
    "    .withColumn (\n",
    "        \"price\", \"double\",\n",
    "        minValue=100,\n",
    "        maxValue=1000,\n",
    "        random=True\n",
    "    )\n",
    "    .withColumn (\n",
    "        \"purchase_location\", \"string\",\n",
    "        values=[\"website\", \"app\", \"in-person\"],\n",
    "        random=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build DataFrame from the above specification\n",
    "df = df_spec.build()\n",
    "num_rows = df.count()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f56f78-5d55-4aee-9e13-dea535672a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a Sample Delta Table\n",
    "Here we create a catalog, schema, and demo Delta Lake table to store the generated data, ready for use in later quality validation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9876387d-7938-46a5-9982-d8cde57e01a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set catalog, schema, and table for storing data\n",
    "catalog = \"george_test\"\n",
    "schema = \"dqx\"\n",
    "table_name = \"dqx_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8775a69-a8aa-42e3-a25a-fe4a916c1d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create catalog and schema if they don't exist\n",
    "# Create a fresh Delta table with demo columns\n",
    "\n",
    "spark.sql(\n",
    "    f\"CREATE CATALOG IF NOT EXISTS {catalog}\"\n",
    ")\n",
    "spark.sql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\"\n",
    ")\n",
    "df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339ad5d0-6619-4e28-a157-f36b26e01394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"SELECT * FROM {catalog}.{schema}.{table_name} LIMIT 10\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6444096297400574,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Setup Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
